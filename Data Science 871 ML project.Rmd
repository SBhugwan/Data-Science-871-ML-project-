---
# IMPORTANT: Change settings here, but DO NOT change the spacing.
# Remove comments and add values where applicable.
# The descriptions below should be self-explanatory

title: "Helping You Write Academic Papers in R using Texevier"
#subtitle: "This will appear as Right Header"

documentclass: "elsarticle"

# --------- Thesis title (Optional - set to FALSE by default).
# You can move the details below around as you please.
Thesis_FP: FALSE
# Entry1: "An unbelievable study with a title spanning multiple lines."
# Entry2: "\\textbf{Nico Katzke}" # textbf for bold
# Entry3: "A thesis submitted toward the degree of Doctor of Philosophy"
# Uni_Logo: Tex/Logo.png # Place a logo in the indicated location (from your root, e.g. defaults to ~/Tex/Logo.png) and uncomment this line. Leave uncommented for no image
# Logo_width: 0.3 # If using a logo - use this to set width (size) of image
# Entry4: "Under the supervision of: \\vfill Prof. Joe Smith and Dr. Frank Smith"
# Entry5: "Stellenbosch University"
# Entry6: April 2020
# Entry7:
# Entry8:

# --------- Front Page
# Comment: ----- Follow this pattern for up to 5 authors
AddTitle: TRUE # Use FALSE when submitting to peer reviewed platform. This will remove author names.
Author1: "Nico Katzke^[__Contributions:__  \\newline _The authors would like to thank no institution for money donated to this project. Thank you sincerely._]"  # First Author - note the thanks message displayed as an italic footnote of first page.
Ref1: "Github: https://github.com/SBhugwan/Data-Science-871-ML-project-" # First Author's Affiliation
Email1: "21075492\\@sun.ac.za" # First Author's Email address

  # If corresponding author is author 3, e.g., use CorrespAuthor_3: TRUE

# Comment out below to remove both. JEL Codes only given if keywords also given.


# ----- Manage headers and footers:
#BottomLFooter: $Title$
#BottomCFooter:
#TopLHeader: \leftmark # Adds section name at topleft. Remove comment to add it.
BottomRFooter: "\\footnotesize Page \\thepage" # Add a '#' before this line to remove footer.
addtoprule: TRUE
addfootrule: TRUE               # Use if footers added. Add '#' to remove line.

# --------- page margins:
margin: 2.3 # Sides
bottom: 2 # bottom
top: 2.5 # Top
HardSet_layout: TRUE # Hard-set the spacing of words in your document. This will stop LaTeX squashing text to fit on pages, e.g.
# This is done by hard-setting the spacing dimensions. Set to FALSE if you want LaTeX to optimize this for your paper.

# --------- Line numbers
linenumbers: FALSE # Used when submitting to journal

# ---------- References settings:
# You can download cls format here: https://www.zotero.org/ - simply search for your institution. You can also edit and save cls formats here: https://editor.citationstyles.org/about/
# Hit download, store it in Tex/ folder, and change reference below - easy.
bibliography: Tex/ref.bib       # Do not edit: Keep this naming convention and location.
csl: Tex/harvard-stellenbosch-university.csl # referencing format used.
# By default, the bibliography only displays the cited references. If you want to change this, you can comment out one of the following:
#nocite: '@*' # Add all items in bibliography, whether cited or not
# nocite: |  # add specific references that aren't cited
#  @grinold2000
#  @Someoneelse2010

# ---------- General:
RemovePreprintSubmittedTo: TRUE  # Removes the 'preprint submitted to...' at bottom of titlepage
Journal: "Journal of Finance"   # Journal that the paper will be submitting to, if RemovePreprintSubmittedTo is set to TRUE.
toc: FALSE                       # Add a table of contents
numbersections: TRUE             # Should sections (and thus figures and tables) be numbered?
fontsize: 12pt                  # Set fontsize
linestretch: 1.5               # Set distance between lines.
link-citations: TRUE            # This creates dynamic links to the papers in reference list.

### Adding additional latex packages:
# header-includes:
#    - \usepackage{colortbl} # Add additional packages here.

output:
  pdf_document:
    keep_tex: TRUE
    template: Tex/TexDefault.txt
    fig_width: 3.5 # Adjust default figure sizes. This can also be done in the chunks of the text.
    fig_height: 3.5
abstract: |
  Abstract to be written here. The abstract should not be too long and should provide the reader with a good understanding what you are writing about. Academic papers are not like novels where you keep the reader in suspense. To be effective in getting others to read your paper, be as open and concise about your findings here as possible. Ideally, upon reading your abstract, the reader should feel he / she must read your paper in entirety.
---

<!-- First: Set your default preferences for chunk options: -->

<!-- If you want a chunk's code to be printed, set echo = TRUE. message = FALSE stops R printing ugly package loading details in your final paper too. I also suggest setting warning = FALSE and checking for warnings in R, else you might find ugly warnings in your paper. -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 5, fig.pos="H", fig.pos = 'H')
# Note: Include = FALSE implies the code is executed, but not printed in your pdf.
# warning and message = FALSE implies ugly messages and warnings are removed from your pdf.
# These should be picked up when you execute the command chunks (code sections below) in your rmd, not printed in your paper!

# Lets load in example data, and see how this can be stored and later called from your 'data' folder.
if(!require("tidyverse")) install.packages("tidyverse")
library(tidyverse)


# Notice that as you are working in a .Rproj file (I am assuming you are) - the relative paths of your directories start at your specified root.
# This means that when working in a .Rproj file, you never need to use getwd() - it is assumed as your base root automatically.
source("/Users/sahilbhugwan/Downloads/Data science/Data Science 871 ML project/code/Loading data.R")

#Data exploration 
source("/Users/sahilbhugwan/Downloads/Data science/Data Science 871 ML project/code/Data Exploration.R")

source("/Users/sahilbhugwan/Downloads/Data science/Data Science 871 ML project/code/feature engineering and prediction.R")


```

<!-- ############################## -->

<!-- # Start Writing here: -->

<!-- ############################## -->

# Introduction \label{Introduction}

# Idea

The main idea of this project is to try and predict who should have won the 2022 FIFA world cup in Qatar. My idea was to look at all the past international matches that has been played since 1872. The reason for this is sometimes there tends to be patterns that emerge, as in sports there is an old saying that history repeats itself. As well as when one views sport on TV many analysts tend to compare current matches to past ones. I then will also look at the ranking of the teams in the World cup as that usual as some indication on how well a team is doing leading to a world cup. This is because if a team is ranking high it generally indicates that they are performing well and winning consistently.

# Data

The data that i will be using for this project will be all the international soccer matches which i found on Kaggle. This data set includes all the matches that was played it has both home and away team, there scores as well as the tournament that the match was being played. Given that this data set included data on all soccer matches up an till after the world cup i then filtered the data to just before the world cup as i want to predict the matches in the world cup based on past performance. The next data set i loaded was the FIFA world cup ranking. This data set contains the ranking of teams since its inception in 1992. It is updated based on when international matches therefore there is no consistent pattern to when it was updated. After that i then converted the data to date time as well as filtered the data from August 1, 2018, onward. Given that certain teams in the data set have different names i then filtered it so that there is consistency throughout. I then merged the data set with the games played. Below is an example using Brazil as an example.

```{r Figure1,  warning =  FALSE, fig.align = 'center', fig.cap = "Brazil matches  \\label{Figure1}", fig.ext = 'png', fig.height = 3, fig.width = 6}


tail(df_wc_ranked[df_wc_ranked$home_team == "Brazil" | df_wc_ranked$away_team == "Brazil", ], 10)
```

Lastly i then loaded the FIFA world cup matches that will be played in Qatar as well as the teams and group allocations.

# Data Exploration

In this section i will do some analysis on the past 150 years of soccer matches. During the international season international teams play a wide range of different games that include friendly matches, world cup/ continent qualifiers as well specific tournaments. However one must take note that the FIFA world cup only takes places every 4 years and if a team plays all the group stage matches and all the knock out matches then the maximum games they will play will be 7 matches therefore in total a FIFA world cup only consists of 64 games every 4 years. Therefore in a 4 year cycle majority of international matches will be be friendly games.

```{r Figure2,  warning =  FALSE, fig.align = 'center', fig.cap = "Annual Matches  \\label{Figure2}", fig.ext = 'png', fig.height = 6, fig.width = 7}

Annualmatches

```

as seen in Figure \ref{Figure2} it clear that majority of the games that a team plays is friendlies. FIFA world cup qualifiers as well as the teams continent qualifiers are the second most matches that international teams will play. This can be examined more clearly on the figure below.

```{r Figure3,  warning =  FALSE, fig.align = 'center', fig.cap = "Break down of matches played   \\label{Figure3}", fig.ext = 'png', fig.height = 4, fig.width = 6}

MajorM
```

Therefore one can see that it is vital that teams perform well in qulifications and friendly matches as it will allow the team to build moment to going into a major tournament. As when a team makes the tournament they play a handful of games and once they make the knock out matches there is no room for error.

Given that teams play a lot of friendly games one most ask the question how how important these games are in the context of winning a world cup. Intuitively it makes sense that the qualification games are important vital, as a team has to qualify for a world cup in order to win a world cup. looking at the continental tournaments such as Euro or African cup of nations they are important matches in terms of finding out who the best team in in that specific continent. However those maths don't have an impact on the world cup, as a team can when the that trophy and still not qualify for the world cup. an example of that most recently was Italy that won the Euro in 2021 but failed to qualify for the 2022 world cup. The figure below is a break down of importance of each tournament.

```{r Figure4,  warning =  FALSE, fig.align = 'center', fig.cap = "Importance by tournmnet    \\label{Figure4}", fig.ext = 'png', fig.height = 7, fig.width = 6}

IBT
```

Therefore is a countries sole purpose is to win the FIFA world cup they should be looking to perform there best during the FIFA world cup qualifications therefore increasing there chance of making it it to the world cup. One important consideration that should be noted is that friendlies and any qualification have an impact on the teams rankings in the world, this is a psychological factor that can impact on how the teams perform in crunch matches.

Therefore i will next look at all the previous FIFA world cup winners as illustrated in the graph below.

```{r Figure5,  warning =  FALSE, fig.align = 'center', fig.cap = "FIFA World Cup Winners     \\label{Figure5}", fig.ext = 'png', fig.height = 6, fig.width = 6}

WCW

```

It should be noted that the first FIFA world cup was played in 1932 and is played every 4 years. There was however only two world cups that where played this was due to World War 2. Looking at this graph it is clear that Brazil is by far the best performing team at the world cup. Just for interest Brazil is also the only team in the world to have played at every single world cup since its inception in 1932, which clearly shows that by playing in world cups it does increase the likelyhood of winning the tournament.

# Initial Plan for Feature Engineering 

my initial idea was to to look at a wide range of features to try and predict who should have one the FIFA world cup. To do this i was going to look ranking , points, home/away fixtures as well as a variety of other features. However i ran into a few issues with the code not being able to producing spuriousness results. The code for this can be viewed on github on previous commits

# Feature Engineering 

Given this i decided to simplify it even further I decided to predict the teams survival of making out the group stages of the World cup. Given that 32 teams start the world cup, i wanted to see after the group stage matches with 16 teams will remain. To do this i will only be looking at only a few features that being matches from 1992 the reason why i decided to look from matches from there is due to the fact that teams rankings where first introduced back in 1992. Therefore from this i can see the changes in the rank of the team and work out the average rank of the team these are all done by looking at the total points that a team gets from playing international matches. The other additional features that i will be looking at is where the match that they playing has some level of stake (importance of the game) and it it was played at a world cup.

# Training 

In this section i will discuss what i did with regards to predicting the matches, these features that i mentioned earlier (that being average rank, rank difference , point difference , stake of match and lastly if it was played at a world cup). These features i will be using for training the machine learning models. looking at the code (that is in the feature engineering and prediction) I first split the model into two different sections that being training and testing sets for machine learning. By splitting the data into different sets i can use the X_train and y_train sets to evaluate the performance on the unseen data using the X_test and y_test sets. Therefore by separating these sets will allow for the model to asses it ability with regards to being able to generate new, unseen data that and thereby avoid overfitting with regards to the training data. I did also create a function that ensure that it takes the target variable "y" which ensure that there is a 80%-20% split in favor of the training data. Another important note is that the variable y is the dependent variable in which the machine learning model aims to predict. I set the seed to 42 this to ensure reproducibility by doing this it ensure that i am able to be able to obtain the same train/ test split every time i run the code, this will ensure that my results are consistent and reproducible.

## Models

The next thing that i did was look at different models. The models that i looked at are the following logistic regression, support vector machines, K-nearest neighbors, Gaussian naive Bayes, decision trees and random forests. Therefore each of these models have there different strengths and weakness, by trying different models i am able to compare certain metrics such as the area under the ROC curve which will allow me to identify which models are best suit for the task at hand. Another reason for looking at different models is that each of them has different hyperparameters that has an impact on its performance.

The results of each of the models can be illustrated below.

```{r Figure6,  warning =  FALSE, fig.align = 'center', fig.cap = "Model Comparison     \\label{Figure6}", fig.ext = 'png', fig.height = 7, fig.width = 6}

plot_model_accuracy(models)
```

The results from my different tests clearly show that the logistic regression performs the best out of all the different models. Given this and the fact that accuracy is a common metric used which measures the proportion of correct predictions from the total number of predictions. In all the different models the logistic regression is able to correctly predict the outcome of the match 68.02% of the time on my test set.

### Cross Validation 

A cross validation technique is a re-sampling technique that is used in machine learning that is used to assess the performance of the models. the main idea of behind this to once again split the the data into multiple sets these are refereed to as folds. The model is once again training on some of the data (for example say K folds) and then evaluates on the remaining sets (K-1). This process is then repeated multiple times however it uses different sets of the data this allows for it to serve as the validation each time. The benefits of this is that it it is able to provide a more robust estimate of the model when compared to a single train split, thus allowing for a better understanding of how the model is able to generalize the unseen data. This method is also used with regards to tuning hyperparamters, as it is able to allow to find the optimal combinations of hyperparemters.

I performed two different cross validations the first being of the random forest. I got a bias value of -66.76 what this results means is that the model is in fact performing worse without cross validation. which is unexpected given the fact that cross validation is generally expected to improve the models performance. Therefore what can be concluded from this is that the random forest model without cross validation actually performs better. There are a few possibilities to consider why this may be the case such as data issues meaning that the data used in the cross validation might differ in some ways from the data used to train the random forest model. what i could find is that there was missing values in the prepossessing. Another possibility is the number of folds used (K) which will affect the results. I then ran a test to get the variance which was 0.0003822415. This results indicate that there is a very low variability with regards to to the accuracy obtained in the cross validation process. The positive outcome in my results means that the model is not heavily influenced by the training and validation subsets.

I then did a cross validation with regards to the logistic regression. I however also obtained a negative bias value of -67.34309 this suggests that once again this model performs worse that the normal logistics regression. This once again suggests that there may be issues with the cross performance validation.

# logistic Regression ROC Curve 

The ROC curve show the trade-off between the true positive rate and the false positive rate at different thresholds. For some reason I wasn't able to print the the curve. I was able to calculate the area under the curve in my code it is print(roc_obj) which is 0.7541. What this mean is that my model has some level of discriminatory power. It performs better than a random classifier however it may have room for improvement to achieve a higher accuracy rate.

# Bias Variance trade off 

An two important concept for machine learning are bias and variance. Bias is used to describe the error that occurs when a simplified model is used to approximate a real-world problem. It expresses how well the model's forecasts match the actual values. A model with a high bias makes significant assumptions about the data by oversimplifying it and possibly underfitting the training set. This means that the model is unable to capture underlying patterns and consistently under- or overestimates true values. Low accuracy and poor generalisation to previously unseen data can result from high bias.

Variance refers to the variability or the sensitivity of the models prediction with regards to the fluctuations in the training data. Therefore it measures how well the models predictions will change with regards to the trained data on different subsets. If the model has a high variance it indicates that the model is very sensitive with regards to random fluctuations in the training data which may result in overfitting. This will generally occurs when the model fails to generalize the new unseen data.

```{r}
BVTO
```

The figure shows the bias variance trade off what one can see is that of the number of fold increase there is fluctuations with regards to the accuracy of the model. However it should be noted that the variance tends to fluctuate between 0.67 and 0.6725 which shows that even though there is fluctutaions it tends to be small therefore we can see that that of average the accuracy of the the model is about 0.67125

# Prediction 

The next thing that did was create a function to predict the probability of each of of the group stages matches the results are as followers :

\_\_\_Starting group F:\_\_\_

Morocco vs. Croatia: Croatia wins with 0.60

Morocco vs. Belgium: Belgium wins with 0.66

Morocco vs. Canada: Morocco wins with 0.55

Croatia vs. Belgium: Belgium wins with 0.61

Croatia vs. Canada: Croatia wins with 0.60

Belgium vs. Canada: Belgium wins with 0.65

\_\_\_Starting group C:\_\_\_

Argentina vs. Saudi Arabia: Argentina wins with 0.69

Argentina vs. Mexico: Draw

Argentina vs. Poland: Argentina wins with 0.57

Saudi Arabia vs. Mexico: Mexico wins with 0.73

Saudi Arabia vs. Poland: Poland wins with 0.67

Mexico vs. Poland: Draw

\_\_\_Starting group A:\_\_\_

Senegal vs. Qatar: Senegal wins with 0.62

Senegal vs. Netherlands: Netherlands wins with 0.61

Senegal vs. Ecuador: Senegal wins with 0.59

Qatar vs. Netherlands: Netherlands wins with 0.74

Qatar vs. Ecuador: Ecuador wins with 0.57

Netherlands vs. Ecuador: Netherlands wins with 0.64

\_\_\_Starting group E:\_\_\_

Germany vs. Japan: Draw

Germany vs. Spain: Spain wins with 0.58

Germany vs. Costa Rica: Germany wins with 0.55

Japan vs. Spain: Spain wins with 0.64

Japan vs. Costa Rica: Draw

Spain vs. Costa Rica: Spain wins with 0.58

\_\_\_Starting group H:\_\_\_

Uruguay vs. South Korea: Draw

Uruguay vs. Portugal: Portugal wins with 0.58

Uruguay vs. Ghana: Uruguay wins with 0.69

South Korea vs. Portugal: Portugal wins with 0.65

South Korea vs. Ghana: South Korea wins with 0.62

Portugal vs. Ghana: Portugal wins with 0.71

\_\_\_Starting group B:\_\_\_

Iran vs. England: England wins with 0.63

Iran vs. USA: USA wins with 0.57

Iran vs. Wales: Wales wins with 0.56

England vs. USA: Draw

England vs. Wales: Draw

USA vs. Wales: Draw

\_\_\_Starting group G:\_\_\_

Switzerland vs. Cameroon: Switzerland wins with 0.60

Switzerland vs. Brazil: Brazil wins with 0.63

Switzerland vs. Serbia: Draw

Cameroon vs. Brazil: Brazil wins with 0.75

Cameroon vs. Serbia: Serbia wins with 0.65

Brazil vs. Serbia: Brazil wins with 0.56

\_\_\_Starting group D:\_\_\_

Denmark vs. Tunisia: Denmark wins with 0.55

Denmark vs. France: France wins with 0.59

Denmark vs. Australia: Denmark wins with 0.60

Tunisia vs. France: France wins with 0.68

Tunisia vs. Australia: Draw

France vs. Australia: France wins with 0.63

From these results we can see that some of these results actually happened, however a clear noticeable difference is that during the Saudi Arabia vs Argentina game my results predict that Argentina will win with a probability of 0.70 however we know that this in fact that didn't happened. However it should be noted that predict sports games is rather difficult due to the fact that there are always major upsets in competitions however given that my model did give an accuracy of 68.02% one can conclude that there may be some discrepancies in the predictions.

Given these results i then created a function that will calculated the expected points given the probability of each of the teams results. This was done so that i was able to see the survival of each of the countries that will continue out of the world cup. The graph below show the probability of each of the teams

```{r Figure7,  warning =  FALSE, fig.align = 'center', fig.cap = "Probability of survival     \\label{Figure7}", fig.ext = 'png', fig.height = 7, fig.width = 6}

data <- list(
    c('Netherlands', 0.7207123136586894),
    c('Belgium', 0.7041053684174787),
    c('Portugal', 0.7025008859381351),
    c('France', 0.6893043301262857),
    c('Brazil', 0.6878946534259283),
    c('Spain', 0.6419635779920858),
    c('Argentina', 0.6086463678095058),
    c('Mexico', 0.5868558522665447),
    c('England', 0.5691708021782123),
    c('Serbia', 0.5383228703970194),
    c('Wales', 0.5323556070273117),
    c('Uruguay', 0.5313857116348),
    c('Senegal', 0.5238123555510896),
    c('Croatia', 0.5217642297484982),
    c('Denmark', 0.5151762204409138),
    c('Poland', 0.5128567082594242),
    c('USA', 0.5126970678459446),
    c('Germany', 0.4893859548936812),
    c('Switzerland', 0.45510860867744996),
    c('South Korea', 0.45081127452719727),
    c('Costa Rica', 0.43841533837914887),
    c('Japan', 0.4109739077103775),
    c('Ecuador', 0.4107822279220528),
    c('Australia', 0.3877831877546305),
    c('Morocco', 0.3819955705681707),
    c('Tunisia', 0.3721040502951258),
    c('Iran', 0.36831897628115345),
    c('Canada', 0.3482241700140098),
    c('Qatar', 0.28657994414763255),
    c('Cameroon', 0.25700333565836253),
    c('Ghana', 0.24790543284071997),
    c('Saudi Arabia', 0.22215428639701668)
)

# Extract country names and probabilities
countries <- sapply(data, "[[", 1)
probabilities <- as.numeric(sapply(data, "[[", 2))

# Create a bar plot
barplot(probabilities, names.arg = countries, ylab = "Probability", col = rainbow(length(data)), las = 2, cex.names = 0.8, ylim = c(0, max(probabilities) * 1.2))
title(main = "Probability of Countries", ylab = "Probability")

```

The survival of the top 16 teams are very similar to that of the teams that actually made it out the group. However one thing that my model did predict is that a top team like Germany will not make it out of the group. This in factual fact did happen during the world cup, however a surprise of the the world cup is that Morocco made it all the way to the semi-finally, however in my model it predicted that they wont even make it out of the group stages.
